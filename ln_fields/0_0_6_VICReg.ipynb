{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0a0f5a-6a5f-4ad9-8120-0b44c28fd53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, math\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.functional import F\n",
    "import torch.distributions as dist\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "sys.path.append('../')\n",
    "from utils_modules.models import SummaryNet, Expander, Net, vector_to_Cov\n",
    "from utils_modules.vicreg import vicreg_loss\n",
    "import utils_modules.data as utils_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd65e85-10e5-4953-a689-0be3c990a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select device; use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device: %s'%(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f65f4cf-e5e3-45a7-8a0b-23d89db5d0e9",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80670796-4f16-49fd-9c36-b079310f74a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load maps and parameters\n",
    "maps      = np.load(...)[:, :, None, :, :]\n",
    "dset_size = maps.shape[0] # data set size\n",
    "splits    = maps.shape[1] # number of augmentations/views per parameter set\n",
    "\n",
    "params  = np.load(...)[:, None, :]\n",
    "params  = np.repeat(params, splits, axis = 1) # reshape the parameters to match the shape of the maps\n",
    "\n",
    "# pre-process the maps data set\n",
    "rescale     = True\n",
    "standardize = True\n",
    "verbose     = True\n",
    "\n",
    "if rescale:\n",
    "    maps = np.log(maps+1)\n",
    "if standardize:\n",
    "    maps_mean, maps_std = np.mean(maps, dtype=np.float64), np.std(maps, dtype=np.float64)\n",
    "    maps = (maps - maps_mean)/maps_std\n",
    "    \n",
    "if verbose:\n",
    "    print('Shape of parameters and maps:', params.shape, maps.shape)\n",
    "    print('Parameter 1 range of values: [{:.3f}, {:.3f}]'.format(params[:, :, 0].min(), params[:, :, 0].max()))\n",
    "    print('Parameter 2 range of values: [{:.3f}, {:.3f}]'.format(params[:, :, 1].min(), params[:, :, 1].max()))\n",
    "    \n",
    "    if rescale: print('Rescale: ', rescale)\n",
    "    if standardize: print('Standardize: ', standardize)\n",
    "\n",
    "maps   = torch.tensor(maps).float().to(device) \n",
    "params = torch.tensor(params).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9918128d-7ebe-4600-b263-c00931a7994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the data into train, validation, and test sets\n",
    "batch_size = 200\n",
    "train_frac, valid_frac, test_frac = 0.8, 0.1, 0.1\n",
    "\n",
    "\n",
    "train_dset, valid_dset, test_dset = utils_data.create_datasets(maps, params, \n",
    "                                                    train_frac, valid_frac, test_frac, \n",
    "                                                    seed = seed, VICReg=True)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dset, batch_size, shuffle = True)\n",
    "valid_loader = DataLoader(valid_dset, batch_size, shuffle = True)\n",
    "test_loader  = DataLoader(test_dset, batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23456873-9863-4eb8-a762-7a0eec85a7a3",
   "metadata": {},
   "source": [
    "## Train the encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ef014e5-5868-41d8-bc76-264e6671be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fmodel, floss, \n",
    "                 net, mlp_net, \n",
    "                 optimizer, scheduler, \n",
    "                 train_loader, valid_loader,\n",
    "                 batch_size, epochs, splits, \n",
    "                 inv_weight = 1, var_weight = 0, cov_weight = 0):\n",
    "    \n",
    "    # compute minimum validation loss\n",
    "    net.eval() \n",
    "    mlp_net.eval()\n",
    "    total_loss, points = 0., 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in valid_loader:\n",
    "            bsz = x.shape[0] # batch size\n",
    "            \n",
    "            # get VICReg summaries of the two maps\n",
    "            emb_q = mlp_net(net(x[:, 0].contiguous()))\n",
    "            emb_k = mlp_net(net(x[:, 1].contiguous()))\n",
    "            \n",
    "            # compute VICReg loss\n",
    "            loss, inv, var, cov = vicreg_loss(emb_q, emb_k, inv_weight, var_weight, cov_weight)\n",
    "            \n",
    "            total_loss += loss.detach()*bsz\n",
    "            points += bsz\n",
    "\n",
    "    min_loss_valid = total_loss/points\n",
    "    if verbose: print('Min validation loss: ', min_loss_valid)\n",
    "    \n",
    "    # loop over the epochs\n",
    "    for epoch in range(epochs): \n",
    "        \n",
    "        total_loss, points = 0., 0\n",
    "        inv_loss, var_loss, cov_loss = 0., 0., 0.\n",
    "        \n",
    "        net.train()\n",
    "        mlp_net.train()\n",
    "        for x, y in train_loader:\n",
    "            bsz = x.shape[0] # batch size\n",
    "            \n",
    "            # get VICReg summaries of the two maps\n",
    "            emb_q = mlp_net(net(x[:, 0].contiguous()))\n",
    "            emb_k = mlp_net(net(x[:, 1].contiguous()))\n",
    "            \n",
    "            # compute VICReg loss\n",
    "            loss, inv, var, cov = vicreg_loss(emb_q, emb_k, inv_weight, var_weight, cov_weight)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.detach()*bsz\n",
    "            points += bsz\n",
    "            \n",
    "            # compute different components of the loss\n",
    "            inv_loss += inv.detach()*bsz\n",
    "            var_loss += var.detach()*bsz\n",
    "            cov_loss += cov.detach()*bsz\n",
    "                \n",
    "        # get the training loss and its components    \n",
    "        loss_train = total_loss/points\n",
    "        inv_loss   = inv_loss/points\n",
    "        var_loss   = var_loss/points\n",
    "        cov_loss   = cov_loss/points\n",
    "               \n",
    "        # validation\n",
    "        net.eval() \n",
    "        mlp_net.eval()\n",
    "        total_loss, points = 0., 0\n",
    "        inv_loss, var_loss, cov_loss = 0., 0., 0.\n",
    "        with torch.no_grad():\n",
    "            for x, y in valid_loader:\n",
    "                bs = x.shape[0] # batch size\n",
    "                \n",
    "                # get VICReg summaries of the two maps\n",
    "                emb_q = mlp_net(net(x[:, 0].contiguous()))\n",
    "                emb_k = mlp_net(net(x[:, 1].contiguous()))\n",
    "            \n",
    "                # compute VICReg loss\n",
    "                loss, inv, var, cov = vicreg_loss(emb_q, emb_k, inv_weight, var_weight, cov_weight)\n",
    "                total_loss += loss.detach()*bsz\n",
    "                points += bsz\n",
    "                \n",
    "                inv_loss += inv.detach()*bsz\n",
    "                var_loss += var.detach()*bsz\n",
    "                cov_loss += cov.detach()*bsz\n",
    "                \n",
    "        # get validation loss and its components      \n",
    "        loss_valid = total_loss/points\n",
    "        inv_loss   = inv_loss/points\n",
    "        var_loss   = var_loss/points\n",
    "        cov_loss   = cov_loss/points\n",
    "\n",
    "        # save model which performs best on validation set\n",
    "        if loss_valid < min_loss_valid:\n",
    "            if verbose:\n",
    "                print('saving model;  epoch %d; %.4e %.4e'%(epoch, loss_train, loss_valid))\n",
    "            torch.save(net.state_dict(), fmodel)\n",
    "            min_loss_valid = loss_valid\n",
    "        else:\n",
    "            if verbose:\n",
    "                print('epoch %d; %.4e %.4e'%(epoch,loss_train,loss_valid))\n",
    "\n",
    "        if epoch == 0:\n",
    "            f = open(fout, 'w')\n",
    "        else:\n",
    "            f = open(fout, 'a')\n",
    "        f.write('%d %.4e %.4e %.4e %.4e %.4e\\n'%(epoch, loss_train, loss_valid, \n",
    "                                                 inv_loss, var_loss, cov_loss))\n",
    "        f.close()\n",
    "        scheduler.step(loss_valid)\n",
    "        \n",
    "    return net, mlp_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec2835e-ceef-4630-b87f-13ac600f30b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights for the loss function (invariance, variance, covariance)\n",
    "inv_arr = [5]\n",
    "var_arr = [5]\n",
    "cov_arr = [1]\n",
    "\n",
    "# hyperparameters\n",
    "lr         = 2e-4\n",
    "epochs     = 200\n",
    "hidden     = 8\n",
    "last_layer = 2*hidden\n",
    "for num_config in range(len(inv_arr)):\n",
    "    \n",
    "    \n",
    "    # define the encoder model\n",
    "    model = SummaryNet(hidden = hidden, last_layer = last_layer).to(device)\n",
    "    \n",
    "    # define the expander model\n",
    "    mlp_exp_units = [4*last_layer, 4*last_layer]\n",
    "    expander_net = Expander(mlp_exp_units, last_layer, bn = True).to(device)\n",
    "\n",
    "    # define the optimizer, scheduler\n",
    "    optimizer = torch.optim.AdamW([*model.parameters(), *expander_net.parameters()], \n",
    "                                 lr=lr, betas=(0.9, 0.999), eps=1e-8, amsgrad=False)  \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3)\n",
    "\n",
    "    save_dir = ...\n",
    "    inv, var, cov = inv_arr[num_config], var_arr[num_config], cov_arr[num_config]\n",
    "    fmodel = save_dir + 'model_{:d}_{:d}_{:d}.pt'.format(inv, var, cov, num_sims_k)\n",
    "    fout   = save_dir + 'losses_{:d}_{:d}_{:d}.txt'.format(inv, var, cov, num_sims_k)\n",
    "\n",
    "    net, mlp = run_training(fmodel, fout, \n",
    "                            model, expander_net, \n",
    "                            optimizer, scheduler, \n",
    "                            train_loader, valid_loader,\n",
    "                            batch_size=batch_size, epochs=epochs, splits=splits,\n",
    "                            inv_weight = inv, var_weight = var, cov_weight = cov)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd0c24e-3614-4fca-a6ea-921ff4f167da",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = np.loadtxt(fout)\n",
    "start_epoch = 0\n",
    "plt.plot(losses[start_epoch:, 0], losses[start_epoch:, 1], label = 'Training loss')\n",
    "plt.plot(losses[start_epoch:, 0], losses[start_epoch:, 2], label = 'Validation loss')\n",
    "plt.legend(loc = 'best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb40b650-2ad7-4d83-9a1f-b99fc069bee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859397a9-98cc-402c-845f-21d194a7c163",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self_supervised_env",
   "language": "python",
   "name": "self_supervised_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
