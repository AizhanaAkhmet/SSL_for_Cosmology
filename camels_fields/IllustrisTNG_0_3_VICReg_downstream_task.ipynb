{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1b3290-40fd-4e06-a4ae-1ce81a1ddae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.functional import F\n",
    "import torch.distributions as dist\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "sys.path.append('../')\n",
    "from utils_modules.models import SummaryNet, Expander, Net, vector_to_Cov\n",
    "from utils_modules.vicreg import vicreg_loss, vicreg_loss_pairs\n",
    "import utils_modules.data as utils_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcc4a6a-c695-4f76-bf17-290a38166c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device: %s'%(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f65f4cf-e5e3-45a7-8a0b-23d89db5d0e9",
   "metadata": {},
   "source": [
    "## Load maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a8b79d-c34d-45e2-84ae-23047d8b41b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load maps and parameters\n",
    "n_params = 2\n",
    "home_dir = ... # maps and parameters directory\n",
    "maps   = np.load(home_dir + 'Maps_Mtot_IllustrisTNG_LH_z=0.00.npy')\n",
    "dset_size = 1000 # data set size\n",
    "splits    = 15   # number of realizations per parameter set\n",
    "maps_size = maps.shape[-1]\n",
    "maps   = maps.reshape(dset_size, splits, 1, maps_size, maps_size) # prepare maps for VICReg\n",
    "\n",
    "params = np.loadtxt(home_dir + 'params_IllustrisTNG.txt')[:, None, :n_params]\n",
    "params  = np.repeat(params, splits, axis = 1) # reshape the parameters to match the shape of the maps\n",
    "minimum = np.array([0.1, 0.6])\n",
    "maximum = np.array([0.5, 1.0])\n",
    "params  = (params - minimum)/(maximum - minimum) # rescale parameters\n",
    "\n",
    "# pre-process the maps data set\n",
    "rescale     = True\n",
    "standardize = True\n",
    "verbose     = True\n",
    "\n",
    "if rescale:\n",
    "    maps = np.log10(maps)\n",
    "if standardize:\n",
    "    maps_mean, maps_std = np.mean(maps, dtype=np.float64), np.std(maps, dtype=np.float64)\n",
    "    maps = (maps - maps_mean)/maps_std\n",
    "    \n",
    "if verbose:\n",
    "    print('Shape of parameters and maps:', params.shape, maps.shape)\n",
    "    print('Parameter 1 range of values: [{:.3f}, {:.3f}]'.format(params[:, :, 0].min(), params[:, :, 0].max()))\n",
    "    print('Parameter 2 range of values: [{:.3f}, {:.3f}]'.format(params[:, :, 1].min(), params[:, :, 1].max()))\n",
    "    \n",
    "    if rescale: print('Rescale: ', rescale)\n",
    "    if standardize: print('Standardize: ', standardize)\n",
    "\n",
    "maps   = torch.tensor(maps).float().to(device) \n",
    "params = torch.tensor(params).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31694ba8-9b8b-4206-bbe8-bdd31c57eb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the data into train, validation, and test sets\n",
    "seed =1\n",
    "train_frac, valid_frac, test_frac = 0.7, 0.2, 0.1\n",
    "batch_size = 200\n",
    "train_dset, valid_dset, test_dset = utils_data.create_datasets(maps, params, \n",
    "                                                    train_frac, valid_frac, test_frac, \n",
    "                                                    seed = seed, rotations=False)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dset, batch_size, shuffle = True)\n",
    "valid_loader = DataLoader(valid_dset, batch_size, shuffle = True)\n",
    "test_loader  = DataLoader(test_dset, batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23456873-9863-4eb8-a762-7a0eec85a7a3",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0efb12a3-cc3d-4985-b5a2-0d1aa794f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "last_layer = 128\n",
    "inv, var, cov = 25, 25, 1\n",
    "\n",
    "n_pairs = 5\n",
    "lr = 1e-3\n",
    "save_dir    = ...\n",
    "fmodel = save_dir + 'model_{:d}_{:d}_{:d}_n_pairs_{:}_lr_{:.2e}.pt'.format(inv, var, cov, n_pairs, lr)\n",
    "fout   = save_dir + 'losses_{:d}_{:d}_{:d}_n_pairs_{:}_lr_{:.2e}.txt'.format(inv, var, cov, n_pairs, lr)\n",
    "        \n",
    "model = torchvision.models.resnet18(num_classes=last_layer)\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model.to(device);\n",
    "\n",
    "model.load_state_dict(torch.load(fmodel))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c282aad5-3f47-462c-99cd-a18e737ce318",
   "metadata": {},
   "source": [
    "## Convert maps into summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "658e2b28-a854-42c3-9ffc-b2396698529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "x_valid = []\n",
    "y_valid = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in train_loader:\n",
    "        x    = x.to(device=device)\n",
    "        y    = y.to(device=device)\n",
    "        x_NN = model(x).to(device=device)\n",
    "        \n",
    "        x_train.append(x_NN)\n",
    "        y_train.append(y)\n",
    "        \n",
    "    for x, y in valid_loader:\n",
    "        x    = x.to(device=device)\n",
    "        y    = y.to(device=device)\n",
    "        x_NN = model(x).to(device=device)\n",
    "        \n",
    "        x_valid.append(x_NN)\n",
    "        y_valid.append(y)\n",
    "\n",
    "############################\n",
    "x_train = torch.cat(x_train)\n",
    "y_train = torch.cat(y_train)\n",
    "\n",
    "train_dset = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_dset, batch_size, shuffle = True)\n",
    "############################\n",
    "\n",
    "x_valid = torch.cat(x_valid)\n",
    "y_valid = torch.cat(y_valid)\n",
    "\n",
    "valid_dset = TensorDataset(x_valid, y_valid)\n",
    "valid_loader = DataLoader(valid_dset, batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5478b04e-8c29-458e-85a8-e7261d7aa04d",
   "metadata": {},
   "source": [
    "## Downstream task: Run parameter inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d070a124-49ed-4ed8-b956-0bf6008a12ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params   = 2\n",
    "n_tril = int(n_params * (n_params + 1) / 2)  # Number of parameters in lower triangular matrix, for symmetric matrix\n",
    "n_out = n_params + n_tril  # Dummy output of neural network\n",
    "\n",
    "# architecture parameters \n",
    "fmodel_lr = save_dir + 'model_{:d}_{:d}_{:d}_LR_summaries.pt'.format(inv, var, cov)\n",
    "fout_lr   = save_dir + 'losses_{:d}_{:d}_{:d}_LR_summaries.txt'.format(inv, var, cov)\n",
    "\n",
    "# define the expander model\n",
    "mlp_lr_units = [4*last_layer, 4*last_layer, n_out]\n",
    "lr_net = Expander(mlp_lr_units, last_layer, bn = True).to(device)\n",
    "\n",
    "# define the optimizer\n",
    "lr     = 7e-4\n",
    "epochs = 150\n",
    "optimizer = torch.optim.AdamW(lr_net.parameters(), lr=lr, betas=(0.9, 0.999), \n",
    "                             eps=1e-8, amsgrad=False)  \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491fe97c-f655-4dfd-a6b3-dd94b8b50bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the neural network for the downstream task\n",
    "lr_net.eval()\n",
    "min_valid_loss, points = 0.0, 0\n",
    "for x, y in valid_loader:\n",
    "    with torch.no_grad():\n",
    "        x    = x.to(device=device)\n",
    "        y    = y.to(device=device)\n",
    "        y_NN = lr_net(x).to(device=device) \n",
    "        \n",
    "        y_pred, cov_pred = y_NN[:, :n_params], y_NN[:, n_params:]\n",
    "        Cov = vector_to_Cov(cov_pred.cpu()).to(device=device)\n",
    "        loss = -dist.MultivariateNormal(loc=y_pred, covariance_matrix=Cov).log_prob(y).to(device=device).mean()\n",
    "        \n",
    "        min_valid_loss += (loss.cpu().item())*x.shape[0]\n",
    "        points += x.shape[0]\n",
    "        \n",
    "min_valid_loss /= points\n",
    "if verbose:\n",
    "    print('Initial valid loss = %.3e'%min_valid_loss)\n",
    "\n",
    "# loop over the epochs\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # training\n",
    "    train_loss, num_points = 0.0, 0\n",
    "    lr_net.train()\n",
    "    for x,y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_NN = lr_net(x).to(device=device) \n",
    "        \n",
    "        y_pred, cov_pred = y_NN[:, :n_params], y_NN[:, n_params:]\n",
    "        Cov = vector_to_Cov(cov_pred.cpu()).to(device=device)\n",
    "        loss = -dist.MultivariateNormal(loc=y_pred, covariance_matrix=Cov).log_prob(y).mean()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += (loss.cpu().item())*x.shape[0]\n",
    "        num_points += x.shape[0]\n",
    "        \n",
    "    train_loss = train_loss/num_points\n",
    "\n",
    "    # validation\n",
    "    valid_loss, num_points = 0.0, 0\n",
    "    lr_net.eval()\n",
    "    for x,y in valid_loader:\n",
    "        with torch.no_grad():\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)            \n",
    "            y_NN = lr_net(x).to(device=device) \n",
    "        \n",
    "            y_pred, cov_pred = y_NN[:, :n_params], y_NN[:, n_params:]\n",
    "            Cov = vector_to_Cov(cov_pred.cpu()).to(device=device)\n",
    "            loss = -dist.MultivariateNormal(loc=y_pred, covariance_matrix=Cov).log_prob(y).mean()\n",
    "            \n",
    "            valid_loss += (loss.cpu().item())*x.shape[0]\n",
    "            num_points += x.shape[0]\n",
    "    valid_loss = valid_loss/num_points\n",
    "\n",
    "    # verbose\n",
    "    if valid_loss<min_valid_loss:\n",
    "        min_valid_loss = valid_loss\n",
    "        torch.save(lr_net.state_dict(), fmodel_lr)\n",
    "        print('Epoch %d: %.3e %.3e (saving)'%(epoch, train_loss, valid_loss))\n",
    "    else:\n",
    "        print('Epoch %d: %.3e %.3e '%(epoch, train_loss, valid_loss))\n",
    "\n",
    "    if epoch == 0:\n",
    "        f = open(fout_lr, 'w')\n",
    "    else:\n",
    "        f = open(fout_lr, 'a')\n",
    "    f.write('%d %.4e %.4e\\n'%(epoch, train_loss, valid_loss))\n",
    "    f.close()\n",
    "    \n",
    "    scheduler.step(valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab10cc8-341f-4113-8678-5debd841d81f",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f0f17f-d020-4782-a716-6aa82ce0f4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = np.loadtxt(fout_lr)\n",
    "start_epoch = 0\n",
    "end_epoch = 200\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot(losses[start_epoch:end_epoch, 0], losses[start_epoch:end_epoch, 1], label = 'Training loss')\n",
    "plt.plot(losses[start_epoch:end_epoch, 0], losses[start_epoch:end_epoch, 2], label = 'Validation loss')\n",
    "plt.legend(loc = 'best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdedcb6-63e6-42ad-9394-d8f4890af080",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(fmodel))\n",
    "model.eval();\n",
    "\n",
    "lr_net.load_state_dict(torch.load(fmodel_lr))\n",
    "lr_net.eval(); \n",
    "\n",
    "test_loss, num_points = 0., 0\n",
    "params_true = []\n",
    "params_pred = []\n",
    "errors_pred = []\n",
    "with torch.no_grad(): \n",
    "    for x, y in test_loader:\n",
    "        x = x.float()\n",
    "        y = y.float()[:, [0, 1]]\n",
    "        bs = x.shape[0]\n",
    "        \n",
    "        y_NN = lr_net(model(x))\n",
    "        \n",
    "        \n",
    "        y_pred, cov_pred = y_NN[:, :n_params], y_NN[:, n_params:]\n",
    "        Cov = vector_to_Cov(cov_pred.cpu()).to(device=device)\n",
    "        loss = -dist.MultivariateNormal(loc=y, covariance_matrix=Cov).log_prob(y_pred).mean()\n",
    "        \n",
    "        test_loss += (loss.cpu().item())*x.shape[0]\n",
    "        num_points += x.shape[0]\n",
    "        \n",
    "        params_true.append(y)\n",
    "        params_pred.append(y_pred)\n",
    "        errors_pred.append(Cov)\n",
    "    \n",
    "    test_loss = test_loss/num_points\n",
    "print('Test loss: ', test_loss)\n",
    "\n",
    "params_true = torch.cat(params_true)\n",
    "params_pred = torch.cat(params_pred)  \n",
    "errors_pred = torch.cat(errors_pred)\n",
    "\n",
    "delta_scale = maximum - minimum\n",
    "params_true = params_true.cpu()*(maximum - minimum) + minimum\n",
    "params_pred = params_pred.cpu()*(maximum - minimum) + minimum  \n",
    "errors_pred = np.array([torch.sqrt(errors_pred[:, 0, 0]).cpu().numpy()*delta_scale[0], \n",
    "                        torch.sqrt(errors_pred[:, 1, 1]).cpu().numpy()*delta_scale[1]]).T\n",
    "\n",
    "\n",
    "\n",
    "MSE_error = F.mse_loss(params_true[:, :2], params_pred[:, :2]).cpu().numpy()\n",
    "print('MSE error: {:}'.format(MSE_error))\n",
    "MSE_error = F.mse_loss(params_true[:, :1], params_pred[:, :1]).cpu().numpy()\n",
    "print('MSE error on OmegaM: {:}'.format(MSE_error))\n",
    "MSE_error = F.mse_loss(params_true[:, 1:], params_pred[:, 1:]).cpu().numpy()\n",
    "print('MSE error on sigma8: {:}'.format(MSE_error))\n",
    "\n",
    "print('\\nActual errors on A, B (relative, %)')\n",
    "print((torch.abs(params_pred[:, :1] - params_true[:, :1])/params_true[:, :1]).mean()*100)\n",
    "print((torch.abs(params_pred[:, 1:] - params_true[:, 1:])/params_true[:, 1:]).mean()*100)\n",
    "\n",
    "print('\\nPredicted errors on A, B (relative, %)')\n",
    "print((errors_pred[:, 0]/params_pred[:, :1].cpu()).mean()*100)\n",
    "print((errors_pred[:, 1]/params_pred[:, 1:].cpu()).mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d00263b-68ad-4600-9ccb-fe782576421d",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_pred_plot = params_pred.cpu()\n",
    "params_true_plot = params_true.cpu()\n",
    "\n",
    "params_unique, indices_unique = np.unique(params_true_plot[:, 0], return_index=True)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(indices_unique)\n",
    "indices_unique = indices_unique[:100]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(17, 8))\n",
    "fig.tight_layout()\n",
    "plt.subplots_adjust(wspace=.2)\n",
    "\n",
    "axs[0].set_ylabel('Predicted '+ r'$\\Omega_M$')\n",
    "axs[0].set_xlabel('True '+ r'$\\Omega_M$')\n",
    "axs[0].errorbar(params_true_plot[indices_unique, 0], params_pred_plot[indices_unique, 0], \n",
    "                yerr=errors_pred[indices_unique, 0], c = '#984ea3',\n",
    "                linestyle = '', label =  r'$1\\sigma$',)# marker = 'o')\n",
    "axs[0].plot([0.1, 0.5], [0.1, 0.5], c = 'k', lw = 2)\n",
    "axs[0].legend(loc='best')\n",
    "axs[0].set_aspect('equal')\n",
    "\n",
    "axs[1].set_ylabel('Predicted '+ r'$\\sigma_8$')\n",
    "axs[1].set_xlabel('True '+ r'$\\sigma_8$')\n",
    "axs[1].errorbar(params_true_plot[indices_unique, 1], params_pred_plot[indices_unique, 1], \n",
    "                yerr=errors_pred[indices_unique, 1],  c = '#984ea3',\n",
    "                linestyle = '', label = r'$1\\sigma$',)# marker = 'o')\n",
    "axs[1].plot([0.6, 1.], [0.6, 1.], c = 'k', lw = 2)\n",
    "axs[1].set_aspect('equal')\n",
    "axs[1].legend(loc='best')\n",
    "plt.savefig(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb40b650-2ad7-4d83-9a1f-b99fc069bee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859397a9-98cc-402c-845f-21d194a7c163",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self_supervised_env",
   "language": "python",
   "name": "self_supervised_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
