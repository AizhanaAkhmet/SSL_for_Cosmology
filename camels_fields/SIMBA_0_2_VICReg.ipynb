{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b45daa-3765-4a7f-a7fa-e9681b260c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.functional import F\n",
    "import torch.distributions as dist\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "sys.path.append('../')\n",
    "from utils_modules.models import SummaryNet, Expander, Net, vector_to_Cov\n",
    "from utils_modules.vicreg import vicreg_loss, vicreg_loss_pairs\n",
    "import utils_modules.data as utils_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6173babf-4406-4110-a779-c69fcde19bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device: %s'%(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f65f4cf-e5e3-45a7-8a0b-23d89db5d0e9",
   "metadata": {},
   "source": [
    "## Load maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79f0903-246d-4b4e-8f0e-3f845303eb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load maps and parameters\n",
    "n_params = 2\n",
    "home_dir = ... # maps and parameters directory\n",
    "maps   = np.load(home_dir + 'Maps_Mtot_SIMBA_LH_z=0.00.npy')\n",
    "dset_size = 1000 # data set size\n",
    "splits    = 15   # number of realizations per parameter set\n",
    "maps_size = maps.shape[-1]\n",
    "maps   = maps.reshape(dset_size, splits, 1, maps_size, maps_size) # prepare maps for VICReg\n",
    "\n",
    "params = np.loadtxt(home_dir + 'params_SIMBA.txt')[:, None, :n_params]\n",
    "params  = np.repeat(params, splits, axis = 1) # reshape the parameters to match the shape of the maps\n",
    "minimum = np.array([0.1, 0.6])\n",
    "maximum = np.array([0.5, 1.0])\n",
    "params  = (params - minimum)/(maximum - minimum) # rescale parameters\n",
    "\n",
    "# pre-process the maps data set\n",
    "rescale     = True\n",
    "standardize = True\n",
    "verbose     = True\n",
    "\n",
    "if rescale:\n",
    "    maps = np.log10(maps)\n",
    "if standardize:\n",
    "    maps_mean, maps_std = np.mean(maps, dtype=np.float64), np.std(maps, dtype=np.float64)\n",
    "    maps = (maps - maps_mean)/maps_std\n",
    "    \n",
    "if verbose:\n",
    "    print('Shape of parameters and maps:', params.shape, maps.shape)\n",
    "    print('Parameter 1 range of values: [{:.3f}, {:.3f}]'.format(params[:, :, 0].min(), params[:, :, 0].max()))\n",
    "    print('Parameter 2 range of values: [{:.3f}, {:.3f}]'.format(params[:, :, 1].min(), params[:, :, 1].max()))\n",
    "    \n",
    "    if rescale: print('Rescale: ', rescale)\n",
    "    if standardize: print('Standardize: ', standardize)\n",
    "\n",
    "maps   = torch.tensor(maps).float().to(device) \n",
    "params = torch.tensor(params).float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23456873-9863-4eb8-a762-7a0eec85a7a3",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3829764-4f2e-4df1-a6e2-e5da460b65b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir    = ...\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ef014e5-5868-41d8-bc76-264e6671be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fmodel, floss, \n",
    "                 net, mlp_net, \n",
    "                 optimizer, scheduler, \n",
    "                 train_loader, valid_loader,\n",
    "                 batch_size, epochs, splits, n_pairs = 1,\n",
    "                 inv_weight = 1, var_weight = 0, cov_weight = 0):\n",
    "    \n",
    "    # compute minimum validation loss\n",
    "    net.eval() \n",
    "    mlp_net.eval()\n",
    "    total_loss, points = 0., 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in valid_loader:\n",
    "            bsz = x.shape[0] # get the batch size\n",
    "            \n",
    "            embeds1 = []\n",
    "            embeds2 = []\n",
    "            for pair in range(n_pairs):\n",
    "                id1 = 2*pair\n",
    "                id2 = 2*pair + 1\n",
    "            \n",
    "                emb_q = mlp_net(net(x[:, id1].contiguous()))\n",
    "                emb_k = mlp_net(net(x[:, id2].contiguous()))\n",
    "                \n",
    "                embeds1.append(emb_q)\n",
    "                embeds2.append(emb_k)\n",
    "            \n",
    "            loss, inv, var, cov = vicreg_loss_pairs(embeds1, embeds2, n_pairs, \n",
    "                                                    inv_weight, var_weight, cov_weight)\n",
    "            total_loss += loss.detach()*bsz\n",
    "            points += bsz\n",
    "\n",
    "    min_loss_valid = total_loss/points\n",
    "    if verbose: print('Min validation loss: ', min_loss_valid)\n",
    "    \n",
    "    # loop over the epochs\n",
    "    for epoch in range(epochs): \n",
    "        \n",
    "        total_loss, points = 0., 0\n",
    "        inv_loss, var_loss, cov_loss = 0., 0., 0.\n",
    "        \n",
    "        net.train()\n",
    "        mlp_net.train()\n",
    "        for x, y in train_loader:\n",
    "            bsz = x.shape[0] # get the batch size\n",
    "            \n",
    "            embeds1 = []\n",
    "            embeds2 = []\n",
    "            for pair in range(n_pairs):\n",
    "                id1 = 2*pair\n",
    "                id2 = 2*pair + 1\n",
    "            \n",
    "                emb_q = mlp_net(net(x[:, id1].contiguous()))\n",
    "                emb_k = mlp_net(net(x[:, id2].contiguous()))\n",
    "                \n",
    "                embeds1.append(emb_q)\n",
    "                embeds2.append(emb_k)\n",
    "            \n",
    "            # compute VICReg loss\n",
    "            loss, inv, var, cov = vicreg_loss_pairs(embeds1, embeds2, n_pairs, \n",
    "                                                    inv_weight, var_weight, cov_weight)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.detach()*bsz\n",
    "            points += bsz\n",
    "            \n",
    "            inv_loss += inv.detach()*bsz\n",
    "            var_loss += var.detach()*bsz\n",
    "            cov_loss += cov.detach()*bsz\n",
    "                \n",
    "        # get the training loss and its components    \n",
    "        loss_train = total_loss/points\n",
    "        inv_loss   = inv_loss/points\n",
    "        var_loss   = var_loss/points\n",
    "        cov_loss   = cov_loss/points\n",
    "               \n",
    "\n",
    "        # validation\n",
    "        net.eval() \n",
    "        mlp_net.eval()\n",
    "        total_loss, points = 0., 0\n",
    "        inv_loss, var_loss, cov_loss = 0., 0., 0.\n",
    "        with torch.no_grad():\n",
    "            for x, y in valid_loader:\n",
    "                embeds1 = []\n",
    "                embeds2 = []\n",
    "                for pair in range(n_pairs):\n",
    "                    id1 = 2*pair\n",
    "                    id2 = 2*pair + 1\n",
    "\n",
    "                    emb_q = mlp_net(net(x[:, id1].contiguous()))\n",
    "                    emb_k = mlp_net(net(x[:, id2].contiguous()))\n",
    "\n",
    "                    embeds1.append(emb_q)\n",
    "                    embeds2.append(emb_k)\n",
    "\n",
    "                # compute VICReg loss\n",
    "                loss, inv, var, cov = vicreg_loss_pairs(embeds1, embeds2, n_pairs, \n",
    "                                                        inv_weight, var_weight, cov_weight)\n",
    "\n",
    "                total_loss += loss.detach()*bsz\n",
    "                points += bsz\n",
    "                \n",
    "                inv_loss += inv.detach()*bsz\n",
    "                var_loss += var.detach()*bsz\n",
    "                cov_loss += cov.detach()*bsz\n",
    "                \n",
    "        # get the validation loss and its components      \n",
    "        loss_valid = total_loss/points\n",
    "        inv_loss   = inv_loss/points\n",
    "        var_loss   = var_loss/points\n",
    "        cov_loss   = cov_loss/points\n",
    "\n",
    "        # save model if it is better\n",
    "        if loss_valid < min_loss_valid:\n",
    "            if verbose:\n",
    "                print('saving model;  epoch %d; %.4e %.4e'\\\n",
    "                      %(epoch, loss_train, loss_valid))\n",
    "            torch.save(net.state_dict(), fmodel)\n",
    "            min_loss_valid = loss_valid\n",
    "        else:\n",
    "            if verbose:\n",
    "                print('epoch %d; %.4e %.4e'\\\n",
    "                      %(epoch,loss_train,loss_valid))\n",
    "\n",
    "        if epoch == 0:\n",
    "            f = open(fout, 'w')\n",
    "        else:\n",
    "            f = open(fout, 'a')\n",
    "        f.write('%d %.4e %.4e %.4e %.4e %.4e\\n'%(epoch, loss_train, loss_valid, \n",
    "                                                 inv_loss, var_loss, cov_loss))\n",
    "        f.close()\n",
    "        scheduler.step(loss_valid)\n",
    "        \n",
    "    return net, mlp_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b012971f-17c2-4378-9086-827157f96f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_arr = [25] \n",
    "var_arr = [25] \n",
    "cov_arr = [1] \n",
    "\n",
    "# divide the data into train, validation, and test sets\n",
    "seed = 1\n",
    "batch_size = 50\n",
    "train_frac, valid_frac, test_frac = 0.7, 0.2, 0.1\n",
    "n_views = 10\n",
    "n_pairs = n_views // 2\n",
    "train_dset, valid_dset, test_dset = utils_data.create_datasets(maps, params, \n",
    "                                                    train_frac, valid_frac, test_frac, \n",
    "                                                    seed = seed, VICReg=True, n_views=n_views)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dset, batch_size, shuffle = True)\n",
    "valid_loader = DataLoader(valid_dset, batch_size, shuffle = True)\n",
    "test_loader  = DataLoader(test_dset, batch_size, shuffle = False)\n",
    "\n",
    "if verbose: print('\\n Split the data into train, validation, and test sets.')\n",
    "######################################\n",
    "for num_config in range(len(inv_arr)):\n",
    "    lr         = 1e-3\n",
    "    epochs     = 150\n",
    "    \n",
    "    # define the model\n",
    "    last_layer = 128\n",
    "    \n",
    "    model = torchvision.models.resnet18(num_classes=last_layer)\n",
    "    model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    model.to(device);\n",
    "\n",
    "    # define the expander model\n",
    "    mlp_exp_units = [4*last_layer, 4*last_layer]\n",
    "    expander_net = Expander(mlp_exp_units, last_layer, bn = True).to(device)\n",
    "\n",
    "    # define the optimizer, scheduler\n",
    "    optimizer = torch.optim.AdamW([*model.parameters(), *expander_net.parameters()], \n",
    "                                 lr=lr, betas=(0.9, 0.999), eps=1e-8, amsgrad=False)  \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                           factor=0.3, verbose=True)\n",
    "\n",
    "\n",
    "    inv, var, cov = inv_arr[num_config], var_arr[num_config], cov_arr[num_config]\n",
    "    fmodel = save_dir + 'model_{:d}_{:d}_{:d}_n_pairs_{:}_lr_{:.2e}.pt'.format(inv, var, cov, n_pairs, lr)\n",
    "    fout   = save_dir + 'losses_{:d}_{:d}_{:d}_n_pairs_{:}_lr_{:.2e}.txt'.format(inv, var, cov, n_pairs, lr)\n",
    "    \n",
    "    net, mlp = run_training(fmodel, fout, \n",
    "                            model, expander_net, \n",
    "                            optimizer, scheduler, \n",
    "                            train_loader, valid_loader, n_pairs=n_pairs,\n",
    "                            batch_size=batch_size, epochs=epochs, splits=splits,\n",
    "                            inv_weight = inv, var_weight = var, cov_weight = cov)\n",
    "    print('Done with config = {:d}'.format(num_config+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71422211-7eff-415f-be06-70e1e8c0a63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot VICReg loss functions\n",
    "losses = np.loadtxt(fout)\n",
    "start_epoch = 0\n",
    "end_epoch = 200\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot(losses[start_epoch:end_epoch, 0], losses[start_epoch:end_epoch, 1], label = 'Training loss')\n",
    "plt.plot(losses[start_epoch:end_epoch, 0], losses[start_epoch:end_epoch, 2], label = 'Validation loss')\n",
    "plt.legend(loc = 'best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e5b07c-e5b2-4ac7-929c-f757e27f97e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eb91ac-94ab-4bd4-9746-da94aadfd343",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self_supervised_env",
   "language": "python",
   "name": "self_supervised_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
